{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\coding\\pythonLearning\\MachineLearning\\ImageClassification\\imageClassification\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pathlib \n",
    "import textwrap \n",
    "import google.generativeai as genai\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "import pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  405k  100  405k    0     0  4719k      0 --:--:-- --:--:-- --:--:-- 4822k\n"
     ]
    }
   ],
   "source": [
    "!curl -o image.jpg https://t0.gstatic.com/licensed-image?q=tbn:ANd9GcQ_Kevbk21QBRy-PgB4kQpS79brbmmEG7m3VOTShAn4PecDU5H5UxrJxE3Dw1JiaG17V88QIol19-3TM2wCHw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = pyttsx3.init()\n",
    "\n",
    "rate = engine.getProperty('rate')   # getting details of current speaking rate\n",
    "engine.setProperty('rate', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key='AIzaSyCNy4eyDCKbxNdGf0Pu14ka8PPG6-8RDIA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigger_gemini_pro(question):\n",
    "    model = genai.GenerativeModel('gemini-pro')\n",
    "    response = model.generate_content(question)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigger_gemini_vision_pro(question, image):\n",
    "    model = genai.GenerativeModel('gemini-pro-vision')\n",
    "    response = model.generate_content([question, image])\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tomarkdown(text): \n",
    "    text = text.replace('â€¢', ' *') \n",
    "    return Markdown(textwrap.indent(text, '> ', predicate=lambda : True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image_path, size=(128,128)):\n",
    "    image = PIL.Image.open(image_path)\n",
    "    image_resized = image.resize(size)\n",
    "    return image_resized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-12 23:35:55,462 INFO wav2vec_microphone.main()\n",
      "Some weights of the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-english were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-english and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import torch\n",
    "import pyaudio\n",
    "import numpy as np\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)-15s %(levelname)s %(message)s')\n",
    "logging.info(\"wav2vec_microphone.main()\")\n",
    "\n",
    "model_name = \"jonatasgrosman/wav2vec2-large-xlsr-53-english\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_name).to(device)\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "\n",
    "sampling_rate = 16000\n",
    "record_time_in_seconds = 10\n",
    "number_of_samples = round(record_time_in_seconds * sampling_rate)\n",
    "\n",
    "def stt():\n",
    "    mic_stream = pyaudio.PyAudio().open(format=pyaudio.paInt16,\n",
    "                            channels=1,\n",
    "                            rate=sampling_rate,\n",
    "                            input=True,\n",
    "                            frames_per_buffer=number_of_samples)\n",
    "\n",
    "    # Get audio\n",
    "    logging.info(\"Speak!\")\n",
    "    speech_arr = np.frombuffer(mic_stream.read(number_of_samples), dtype=np.int16)\n",
    "    print(speech_arr)\n",
    "\n",
    "    speech_tsr = torch.from_numpy(speech_arr)\n",
    "    # Tokenize our tensor\n",
    "    input_values = processor(speech_tsr, return_tensord=\"pt\", sampling_rate=sampling_rate)[\"input_values\"]\n",
    "    input_tsr = torch.from_numpy(input_values[0]).to(device).unsqueeze(0)\n",
    "\n",
    "    # Perform inference\n",
    "    logits = model(input_tsr)[\"logits\"]\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    logging.info(f\"type(predicted_ids) = {type(predicted_ids)}; predicted_ids.shape = {predicted_ids.shape}\")\n",
    "\n",
    "    # Decode the IDs to text\n",
    "    transcription = processor.decode(predicted_ids[0]).lower()\n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-12 23:41:48,280 INFO Speak!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 20  10 -39 ...  15  19  23]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-12 23:42:00,809 INFO type(predicted_ids) = <class 'torch.Tensor'>; predicted_ids.shape = torch.Size([1, 499])\n"
     ]
    }
   ],
   "source": [
    "user_input = stt()\n",
    "image_path = 'Chennai-pedestrian.jpg'\n",
    "\n",
    "if image_path:\n",
    "    resized_image = resize_image(image_path)\n",
    "    response = trigger_gemini_vision_pro(user_input, resized_image)\n",
    "else: \n",
    "    response = trigger_gemini_pro(user_input)\n",
    "\n",
    "if (len(response.candidates) > 0):\n",
    "    engine.say(response.text)\n",
    "    engine.runAndWait()\n",
    "else:\n",
    "    engine.say(\"Sorry i can't answer that\")\n",
    "    engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imageClassification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
